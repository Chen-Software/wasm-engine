---
id: ADR-009
title: "Model Caching & GPU Memory Policy"
status: "Proposed"
date: 2025-12-17
tags:
  - onnx
  - gpu
  - performance
---

## 1. Context

[A detailed description of the problem space and the need for a defined strategy for managing ML models, including caching, preloading, and GPU memory allocation, to optimize performance and resource utilization.]

---

## 2. Decision

[A clear statement of the chosen approach, e.g., "The orchestrator's Model Management Service will implement a Least Recently Used (LRU) caching policy for models. GPU memory will be managed by ONNX Runtime, with heuristics for preloading models specified in the agent configuration."]

---

## 3. Rationale

[The reasons behind the decision, referencing key requirements from the PRD such as performance and the central role of the orchestrator.]

---

## 4. Consequences

[The positive and negative outcomes of this decision.]

---

*This is a placeholder document. It will be updated with a detailed analysis and decision.*
